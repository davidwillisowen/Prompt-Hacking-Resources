# JAILBREAKS
---
• [L1B3RT4S](https://github.com/elder-plinius/L1B3RT4S/tree/main) – A GitHub repository with jailbreak prompt sets and tools for evaluating LLM security  
• [Jailbreak Tracker](https://jailbreak-tracker-goochbeaterhs.replit.app/) – Live dashboard tracking known jailbreak prompts across different LLMs  
• [Awesome GPT Super Prompting](https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting) – Curated list of red teaming and jailbreak resources for GPT-style models  
• [Jailbreaking in GenAI: Techniques and Ethical Implications](https://learnprompting.org/docs/prompt_hacking/jailbreaking) – Explores jailbreak methods alongside their ethical and societal concerns  
• [Jailbreaking LLMs: A Comprehensive Guide (With Examples)](https://www.promptfoo.dev/blog/how-to-jailbreak-llms/) – Practical guide offering step-by-step jailbreak prompt examples and techniques  
• [AI Jailbreak – IBM](https://www.ibm.com/think/insights/ai-jailbreak) – Overview of jailbreak threats, implications, and potential mitigation strategies  
• [AI Jailbreaking Demo: How Prompt Engineering Bypasses LLM Security Measures](https://www.youtube.com/watch?v=F_KychntktU) – Video walkthrough demonstrating how prompt engineering can bypass model restrictions  
• [Prompt Injection vs. Jailbreaking: What's the Difference?](https://learnprompting.org/blog/injection_jailbreaking) – Comparison of two related AI exploitation methods: prompt injection vs jailbreaks  
• [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253) – Research paper presenting a framework to auto-generate jailbreak prompts for security testing  
• [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522) – Novel method using diffusion models to create jailbreak prompts for large language models  
• [SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack](https://arxiv.org/abs/2407.01902) – Proposes a jailbreak generation framework leveraging social engineering concepts  
• [Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation](https://arxiv.org/abs/2410.11317) – Introduces techniques to improve jailbreak effectiveness through prompt translation methods  
• [AI Jailbreaks: What They Are and How They Can Be Mitigated](https://www.ibm.com/think/insights/ai-jailbreak) – Additional IBM post explaining jailbreak risks and corporate security approaches
